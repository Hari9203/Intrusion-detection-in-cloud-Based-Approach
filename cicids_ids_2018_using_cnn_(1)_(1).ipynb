{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dad5edc2",
      "metadata": {
        "id": "dad5edc2"
      },
      "source": [
        "# Network Intrusion Detection Using Machine Learning/Deep Learning\n",
        "This notebook involves the making of machine learning & deep learning models to classify the given data of obtained as a network intrusion into differen classes (malignant or benign). Given a sample point, the objective of machine learning model will be to classify that whether the intrusion made is  **Benign** or is a **BruteForce** (either FTP or SSH)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8fe52fb",
      "metadata": {
        "id": "a8fe52fb"
      },
      "source": [
        "# Importing Libraries\n",
        "First, we will import libraries that we need to start our workflow. The libraries we are using are:\n",
        "* NumPy\n",
        "* Pandas\n",
        "* Matplotlib\n",
        "* Scikit-learn\n",
        "* Keras\n",
        "* TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6afcc25",
      "metadata": {
        "id": "a6afcc25"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os, re, time, math, tqdm, itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.offline as pyo\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import keras\n",
        "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\n",
        "from tensorflow.keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6a4f520",
      "metadata": {
        "id": "e6a4f520"
      },
      "outputs": [],
      "source": [
        "# check the available data\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c0f5167",
      "metadata": {
        "id": "0c0f5167"
      },
      "source": [
        "We have a lot of data available to deal with in this notebook. We will perform analysis, preprocessing and modeling on one of the datasets and will conclude the results at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e26f5fd1",
      "metadata": {
        "id": "e26f5fd1"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f32a0d97",
      "metadata": {
        "id": "f32a0d97"
      },
      "source": [
        "# Loading the Data\n",
        "First step is to load the available data into our memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f0dffbc",
      "metadata": {
        "id": "3f0dffbc"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Load the data into memory\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Import pandas and load data from the correct path\n",
        "import pandas as pd\n",
        "\n",
        "# List of file paths to your datasets in Google Drive\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/Intrsuion Detection/02-14-2018.csv',\n",
        "    '/content/drive/MyDrive/Intrusion Detection/02-15-2018.csv',\n",
        "]\n",
        "\n",
        "# Initialize an empty list to hold the individual DataFrames\n",
        "df_list = []\n",
        "\n",
        "# Loop through each file path, read the CSV into a DataFrame, and append it to the list\n",
        "for file_path in file_paths:\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, low_memory=False)  # Suppress the DtypeWarning by reading the file in one go\n",
        "        df_list.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "# Concatenate all DataFrames in the list into a single DataFrame\n",
        "network_data = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Step 4: Check if the data is loaded correctly\n",
        "print(network_data.head())\n",
        "print(\"Total samples loaded:\", network_data.shape[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02628c0e",
      "metadata": {
        "id": "02628c0e"
      },
      "source": [
        "# EDA (Exploratory Data Analysis)\n",
        "For making a proper undertanding of dataset we are using, we will perform a bief EDA (Exploratory Data Analysis). The EDA is sub-divided into:\n",
        "* Data Visuals\n",
        "* Data Understanding\n",
        "* Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26048e11",
      "metadata": {
        "id": "26048e11"
      },
      "outputs": [],
      "source": [
        "# check the shape of data\n",
        "network_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c392fb02",
      "metadata": {
        "id": "c392fb02"
      },
      "outputs": [],
      "source": [
        "# check the number of rows and columns\n",
        "print('Number of Rows (Samples): %s' % str((network_data.shape[0])))\n",
        "print('Number of Columns (Features): %s' % str((network_data.shape[1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aa98ced",
      "metadata": {
        "id": "9aa98ced"
      },
      "source": [
        "We have a total of **1 million+** samples and **80** features in data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f0f366e",
      "metadata": {
        "id": "1f0f366e"
      },
      "outputs": [],
      "source": [
        "network_data.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0482e5d0",
      "metadata": {
        "id": "0482e5d0"
      },
      "outputs": [],
      "source": [
        "# check the columns in data\n",
        "network_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a422cbfa",
      "metadata": {
        "id": "a422cbfa"
      },
      "outputs": [],
      "source": [
        "# check the number of columns\n",
        "print('Total columns in our data: %s' % str(len(network_data.columns)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86287cc4",
      "metadata": {
        "id": "86287cc4"
      },
      "source": [
        "The dataset is huge. We have a total of **80** columns in our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1f79eab",
      "metadata": {
        "id": "c1f79eab"
      },
      "outputs": [],
      "source": [
        "network_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad5c4bb8",
      "metadata": {
        "id": "ad5c4bb8"
      },
      "source": [
        "The following information tells us that:\n",
        "* We have a huge amount of data, containing **1 million+** entries (samples)\n",
        "* There are a total of **80** columns belinging to each sample\n",
        "* There are missing values in our data, which need to be filled or dropped for proper modelling\n",
        "* The memory consumption of data is **700 MB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8130d15d",
      "metadata": {
        "id": "8130d15d"
      },
      "outputs": [],
      "source": [
        "# check the number of values for labels\n",
        "network_data['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddd7624f",
      "metadata": {
        "id": "ddd7624f"
      },
      "source": [
        "Most of the network intrusions in our data are benign, as output from above code cell."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3bdc210",
      "metadata": {
        "id": "e3bdc210"
      },
      "source": [
        "## Data Visualizations\n",
        "After getting some useful information about our data, we now make visuals of our data to see how the trend in our data goes like. The visuals include bar plots, distribution plots, scatter plots, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8782afd4",
      "metadata": {
        "id": "8782afd4"
      },
      "outputs": [],
      "source": [
        "# make a plot number of labels\n",
        "sns.set(rc={'figure.figsize':(12, 6)})\n",
        "plt.xlabel('Attack Type')\n",
        "sns.set_theme()\n",
        "ax = sns.countplot(x='Label', data=network_data)\n",
        "ax.set(xlabel='Attack Type', ylabel='Number of Attacks')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the column names of the dataframe\n",
        "df.columns\n",
        "\n",
        "# Identify categorical data where there are only 2 unique values in the column\n",
        "categorical_data = [row for row in df.columns if len(pd.unique(df[row])) <= 2]\n"
      ],
      "metadata": {
        "id": "-WoyepGrPzBL"
      },
      "id": "-WoyepGrPzBL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histograms of all binary categorical data columns\n",
        "df[categorical_data].hist(figsize=(25, 25))\n"
      ],
      "metadata": {
        "id": "mIBaA9e0P2Bi"
      },
      "id": "mIBaA9e0P2Bi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab2c3de2",
      "metadata": {
        "id": "ab2c3de2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import plotly.offline as pyo\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Plotly in offline mode\n",
        "pyo.init_notebook_mode(connected=True)\n",
        "\n",
        "# Example DataFrame loading (Replace this with your actual DataFrame loading code)\n",
        "# network_data = pd.read_csv('path_to_your_network_data.csv')\n",
        "\n",
        "# Create scatter plot\n",
        "fig = px.scatter(\n",
        "    network_data,\n",
        "    x=\"Bwd Pkts/s\",\n",
        "    y=\"Fwd Seg Size Min\",\n",
        "    title=\"Scatter Plot of Network Data\",\n",
        "    labels={\"Bwd Pkts/s\": \"Bwd Pkts/s\", \"Fwd Seg Size Min\": \"Fwd Seg Size Min\"}\n",
        ")\n",
        "\n",
        "# Show plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f79052",
      "metadata": {
        "id": "23f79052"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "sns.set(rc={'figure.figsize':(12, 6)})\n",
        "sns.scatterplot(x=network_data['Bwd Pkts/s'][:50000], y=network_data['Fwd Seg Size Min'][:50000],\n",
        "                hue='Label', data=network_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "932f8513",
      "metadata": {
        "id": "932f8513"
      },
      "source": [
        "From the graphs, we came to know that:\n",
        "* Most of the attacks made by intruders are malignant (almost 700k)\n",
        "* **FTP-BruteFore** and **SSH-BruteForce** type attacks are less in numbers (less than 200k)\n",
        "* Most of the intruders try to make a malignant attack on network systems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e6af728",
      "metadata": {
        "id": "0e6af728"
      },
      "outputs": [],
      "source": [
        "# check the dtype of timestamp column\n",
        "(network_data['Timestamp'].dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ebd2b84",
      "metadata": {
        "id": "8ebd2b84"
      },
      "source": [
        "# Data Preprocessing\n",
        "Data preprocessing plays an important part in the process of data science, since data may not be fully clean and can contain missing or null values. In this step, we are undergoing some preprocessing steps that will help us if there is any null or missing value in our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "725c5e22",
      "metadata": {
        "id": "725c5e22"
      },
      "outputs": [],
      "source": [
        "# check for some null or missing values in our dataset\n",
        "network_data.isna().sum().to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad305723",
      "metadata": {
        "id": "ad305723"
      },
      "source": [
        "All features in the data have no null or missing values, except one feature that contains **2277** missing values. We need to remove this column from our data, so that our data may get cleaned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3061675a",
      "metadata": {
        "id": "3061675a"
      },
      "outputs": [],
      "source": [
        "# drop null or missing columns\n",
        "cleaned_data = network_data.dropna()\n",
        "cleaned_data.isna().sum().to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88a7e800",
      "metadata": {
        "id": "88a7e800"
      },
      "source": [
        "After removing the missing valued column in our data, we have now no feature that contains any missing or null value. Data is cleaned now."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10b090d5",
      "metadata": {
        "id": "10b090d5"
      },
      "source": [
        "### Label Encoding\n",
        "The Label feature in the data contains 3 labels as **Benign**, **BruteForceFTP** and **BruteForceSSH**. All these are in string format. For our neural network, we need to convert them into numbers so that our NN may understand their representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9baad647",
      "metadata": {
        "id": "9baad647"
      },
      "outputs": [],
      "source": [
        "# encode the column labels\n",
        "label_encoder = LabelEncoder()\n",
        "cleaned_data['Label']= label_encoder.fit_transform(cleaned_data['Label'])\n",
        "cleaned_data['Label'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02f4f0fd",
      "metadata": {
        "id": "02f4f0fd"
      },
      "outputs": [],
      "source": [
        "# check for encoded labels\n",
        "cleaned_data['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ea961e1",
      "metadata": {
        "id": "1ea961e1"
      },
      "source": [
        "## Shaping the data for CNN\n",
        "For applying a convolutional neural network on our data, we will have to follow following steps:\n",
        "* Seperate the data of each of the labels\n",
        "* Create a numerical matrix representation of labels\n",
        "* Apply resampling on data so that can make the distribution equal for all labels\n",
        "* Create X (predictor) and Y (target) variables\n",
        "* Split the data into train and test sets\n",
        "* Make data multi-dimensional for CNN\n",
        "* Apply CNN on data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23e19ca7",
      "metadata": {
        "id": "23e19ca7"
      },
      "outputs": [],
      "source": [
        "# make 3 seperate datasets for 3 feature labels\n",
        "data_1 = cleaned_data[cleaned_data['Label'] == 0]\n",
        "data_2 = cleaned_data[cleaned_data['Label'] == 1]\n",
        "data_3 = cleaned_data[cleaned_data['Label'] == 2]\n",
        "\n",
        "# make benign feature\n",
        "y_1 = np.zeros(data_1.shape[0])\n",
        "y_benign = pd.DataFrame(y_1)\n",
        "\n",
        "# make bruteforce feature\n",
        "y_2 = np.ones(data_2.shape[0])\n",
        "y_bf = pd.DataFrame(y_2)\n",
        "\n",
        "# make bruteforceSSH feature\n",
        "y_3 = np.full(data_3.shape[0], 2)\n",
        "y_ssh = pd.DataFrame(y_3)\n",
        "\n",
        "# merging the original dataframe\n",
        "X = pd.concat([data_1, data_2, data_3], sort=True)\n",
        "y = pd.concat([y_benign, y_bf, y_ssh], sort=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6908646c",
      "metadata": {
        "id": "6908646c"
      },
      "outputs": [],
      "source": [
        "y_1, y_2, y_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "766b4f47",
      "metadata": {
        "id": "766b4f47"
      },
      "outputs": [],
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c302b9da",
      "metadata": {
        "id": "c302b9da"
      },
      "outputs": [],
      "source": [
        "# checking if there are some null values in data\n",
        "X.isnull().sum().to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b2f1572",
      "metadata": {
        "id": "2b2f1572"
      },
      "source": [
        "The output of above cell shows that there are no null values in our data, and the data can now be used for model fitting. We have two types of datasets, normal and abnormal, and they'll be used for model fitting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e13b7175",
      "metadata": {
        "id": "e13b7175"
      },
      "source": [
        "## Data Argumentation\n",
        "Ti avoid biasing in data, we need to use data argumentation on it so that we can remove bias from data and make equal distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00dfea1e",
      "metadata": {
        "id": "00dfea1e"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "data_1_resample = resample(data_1, n_samples=20000,\n",
        "                           random_state=123, replace=True)\n",
        "data_2_resample = resample(data_2, n_samples=20000,\n",
        "                           random_state=123, replace=True)\n",
        "data_3_resample = resample(data_3, n_samples=20000,\n",
        "                           random_state=123, replace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d55c7c94",
      "metadata": {
        "id": "d55c7c94"
      },
      "outputs": [],
      "source": [
        "train_dataset = pd.concat([data_1_resample, data_2_resample, data_3_resample])\n",
        "train_dataset.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19037623",
      "metadata": {
        "id": "19037623"
      },
      "outputs": [],
      "source": [
        "# viewing the distribution of intrusion attacks in our dataset\n",
        "plt.figure(figsize=(10, 8))\n",
        "circle = plt.Circle((0, 0), 0.7, color='white')\n",
        "plt.title('Intrusion Attack Type Distribution')\n",
        "plt.pie(train_dataset['Label'].value_counts(), labels=['Benign', 'BF', 'BF-SSH'], colors=['blue', 'magenta', 'cyan'])\n",
        "p = plt.gcf()\n",
        "p.gca().add_artist(circle)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff893637",
      "metadata": {
        "id": "ff893637"
      },
      "source": [
        "## Making X & Y Variables (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1409d11c",
      "metadata": {
        "id": "1409d11c"
      },
      "outputs": [],
      "source": [
        "test_dataset = train_dataset.sample(frac=0.1)\n",
        "target_train = train_dataset['Label']\n",
        "target_test = test_dataset['Label']\n",
        "target_train.unique(), target_test.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f84209a3",
      "metadata": {
        "id": "f84209a3"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(target_train, num_classes=3)\n",
        "y_test = to_categorical(target_test, num_classes=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ad6c46d",
      "metadata": {
        "id": "0ad6c46d"
      },
      "source": [
        "## Data Splicing\n",
        "This stage involves the data split into train & test sets. The training data will be used for training our model, and the testing data will be used to check the performance of model on unseen dataset. We're using a split of **80-20**, i.e., **80%** data to be used for training & **20%** to be used for testing purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c90c4a48",
      "metadata": {
        "id": "c90c4a48"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.drop(columns = [\"Timestamp\", \"Protocol\",\"PSH Flag Cnt\",\"Init Fwd Win Byts\",\"Flow Byts/s\",\"Flow Pkts/s\", \"Label\"], axis=1)\n",
        "test_dataset = test_dataset.drop(columns = [\"Timestamp\", \"Protocol\",\"PSH Flag Cnt\",\"Init Fwd Win Byts\",\"Flow Byts/s\",\"Flow Pkts/s\", \"Label\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "507b1f17",
      "metadata": {
        "id": "507b1f17"
      },
      "outputs": [],
      "source": [
        "# making train & test splits\n",
        "X_train = train_dataset.iloc[:, :-1].values\n",
        "X_test = test_dataset.iloc[:, :-1].values\n",
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb7e4689",
      "metadata": {
        "id": "eb7e4689"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0327adda",
      "metadata": {
        "id": "0327adda"
      },
      "outputs": [],
      "source": [
        "# reshape the data for CNN\n",
        "X_train = X_train.reshape(len(X_train), X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(len(X_test), X_test.shape[1], 1)\n",
        "X_train.shape, X_test.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a720a3d7",
      "metadata": {
        "id": "a720a3d7"
      },
      "outputs": [],
      "source": [
        "# making the deep learning function\n",
        "def model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu',\n",
        "                    padding='same', input_shape=(72, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # adding a pooling layer\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu',\n",
        "                    padding='same', input_shape=(72, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu',\n",
        "                    padding='same', input_shape=(72, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c62b09b",
      "metadata": {
        "id": "8c62b09b"
      },
      "outputs": [],
      "source": [
        "model = model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f5d8ea",
      "metadata": {
        "id": "28f5d8ea"
      },
      "outputs": [],
      "source": [
        "logger = CSVLogger('logs.csv', append=True)\n",
        "his = model.fit(X_train, y_train, epochs=20, batch_size=32,\n",
        "          validation_data=(X_test, y_test), callbacks=[logger])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "477d070b",
      "metadata": {
        "id": "477d070b"
      },
      "source": [
        "## Visualization of Results (CNN)\n",
        "Let's make a graphical visualization of results obtained by applying CNN to our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c17fc1",
      "metadata": {
        "id": "31c17fc1"
      },
      "outputs": [],
      "source": [
        "# check the model performance on test data\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e8c8baa",
      "metadata": {
        "id": "8e8c8baa"
      },
      "outputs": [],
      "source": [
        "# check history of model\n",
        "history = his.history\n",
        "history.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292bf74b",
      "metadata": {
        "id": "292bf74b"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, len(history['loss']) + 1)\n",
        "acc = history['accuracy']\n",
        "loss = history['loss']\n",
        "val_acc = history['val_accuracy']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "# visualize training and val accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Accuracy (CNN)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(epochs, acc, label='accuracy')\n",
        "plt.plot(epochs, val_acc, label='val_acc')\n",
        "plt.legend()\n",
        "\n",
        "# visualize train and val loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Loss(CNN)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, loss, label='loss', color='g')\n",
        "plt.plot(epochs, val_loss, label='val_loss', color='r')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c81d061",
      "metadata": {
        "id": "6c81d061"
      },
      "source": [
        "# Conclusion after CNN Training\n",
        "After training our deep CNN model on training data and validating it on validation data, it can be interpreted that:\n",
        "* Model was trained on 50 epochs and then on 30 epochs\n",
        "* CNN performed exceptionally well on training data and the accuracy was **99%**\n",
        "* Model accuracy was down to **83.55%** on valiadtion data after **50** iterations, and gave a good accuracy of **92%** after **30** iterations. Thus, it can be interpreted that optimal number of iterations on which this model can perform are **30**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD OPTIMIZATION"
      ],
      "metadata": {
        "id": "PqJ5Kny0jVwS"
      },
      "id": "PqJ5Kny0jVwS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns in the dataset: \", df.columns)\n",
        "\n",
        "# Step 2: Identifying binary categorical columns\n",
        "categorical_data = [row for row in df.columns if len(pd.unique(df[row])) <= 2]\n",
        "print(\"Binary categorical columns: \", categorical_data)\n",
        "\n",
        "# Step 3: Visualizing the distribution of binary categorical columns\n",
        "df[categorical_data].hist(figsize=(25, 25))\n",
        "plt.suptitle('Distribution of Binary Categorical Columns', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5jHTB0EmX31u"
      },
      "id": "5jHTB0EmX31u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SGD optimizer\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Define a new model with SGD optimizer\n",
        "def model_with_sgd():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu',\n",
        "                    padding='same', input_shape=(72, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Adding a pooling layer\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu',\n",
        "                    padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu',\n",
        "                    padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compile the model with SGD optimizer\n",
        "    sgd = SGD(learning_rate=0.01, momentum=0.9)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Instantiate and summarize the model\n",
        "model_sgd = model_with_sgd()\n",
        "model_sgd.summary()\n",
        "\n",
        "# Define a CSV logger for the SGD model\n",
        "logger_sgd = CSVLogger('logs_sgd.csv', append=True)\n",
        "\n",
        "# Make sure labels are one-hot encoded\n",
        "target_train = to_categorical(target_train, num_classes=3)\n",
        "target_test = to_categorical(target_test, num_classes=3)\n",
        "\n",
        "# Check Shapes\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", target_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", target_test.shape)\n",
        "\n",
        "# Train the model with SGD optimizer\n",
        "his_sgd = model_sgd.fit(X_train, target_train, epochs=20, batch_size=32,\n",
        "                        validation_data=(X_test, target_test), callbacks=[logger_sgd])\n",
        "\n",
        "# Evaluate the model performance\n",
        "scores_sgd = model_sgd.evaluate(X_test, target_test)\n",
        "print(\"%s: %.2f%%\" % (model_sgd.metrics_names[1], scores_sgd[1] * 100))\n",
        "\n",
        "# Check the history of the SGD model\n",
        "history_sgd = his_sgd.history\n",
        "history_sgd.keys()\n",
        "\n",
        "# Plot training and validation accuracy for SGD model\n",
        "epochs_sgd = range(1, len(history_sgd['loss']) + 1)\n",
        "acc_sgd = history_sgd['accuracy']\n",
        "loss_sgd = history_sgd['loss']\n",
        "val_acc_sgd = history_sgd['val_accuracy']\n",
        "val_loss_sgd = history_sgd['val_loss']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Accuracy (SGD)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(epochs_sgd, acc_sgd, label='accuracy')\n",
        "plt.plot(epochs_sgd, val_acc_sgd, label='val_acc')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training and validation loss for SGD model\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Loss (SGD)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs_sgd, loss_sgd, label='loss', color='g')\n",
        "plt.plot(epochs_sgd, val_loss_sgd, label='val_loss', color='r')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "ES7H8ztx7blC"
      },
      "id": "ES7H8ztx7blC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Predict the classes for the test set\n",
        "y_pred = model_sgd.predict(X_test)\n",
        "y_pred_classes = y_pred.argmax(axis=-1)\n",
        "y_true_classes = target_test.argmax(axis=-1)\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix (SGD)')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print('Classification Report (SGD):')\n",
        "print(classification_report(y_true_classes, y_pred_classes))\n"
      ],
      "metadata": {
        "id": "ZIFr8y1bFcNC"
      },
      "id": "ZIFr8y1bFcNC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(3):\n",
        "    fpr, tpr, _ = roc_curve(target_test[:, i], y_pred[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LDokowskKzOs"
      },
      "id": "LDokowskKzOs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Plot precision-recall curve for each class\n",
        "for i in range(3):  # Assuming 3 classes\n",
        "    precision, recall, _ = precision_recall_curve(target_test[:, i], model_sgd.predict(X_test)[:, i])\n",
        "\n",
        "    plt.plot(recall, precision, label=f'Class {i}')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve (SGD)')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "d-Jv68qfLjLt"
      },
      "id": "d-Jv68qfLjLt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `y_pred_classes` and `y_true_classes` from before\n",
        "residuals = y_true_classes - y_pred_classes\n",
        "\n",
        "plt.scatter(y_true_classes, residuals)\n",
        "plt.title('Residual Plot (SGD)')\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sp569y7zNBnc"
      },
      "id": "sp569y7zNBnc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the CDF of model predictions\n",
        "import numpy as np\n",
        "\n",
        "y_pred_proba = model_sgd.predict(X_test).max(axis=1)\n",
        "y_pred_sorted = np.sort(y_pred_proba)\n",
        "cdf = np.arange(len(y_pred_sorted)) / float(len(y_pred_sorted))\n",
        "\n",
        "plt.plot(y_pred_sorted, cdf)\n",
        "plt.title('Cumulative Density Function (CDF) of Predictions (SGD)')\n",
        "plt.xlabel('Prediction Probability')\n",
        "plt.ylabel('CDF')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6CePHe6yOjoM"
      },
      "id": "6CePHe6yOjoM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2840.573987,
      "end_time": "2022-12-18T10:55:43.400653",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-12-18T10:08:22.826666",
      "version": "2.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}